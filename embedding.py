import os
import sys
import uuid
import json
from datetime import datetime
import fitz  # PyMuPDF
import requests
from typing import List, Dict, Set
from langchain.text_splitter import RecursiveCharacterTextSplitter
import dotenv
from transformers import GPT2TokenizerFast
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams

# Load env
tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
dotenv.load_dotenv()

OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
QDRANT_URL = os.getenv('QDRANT_URL')
QDRANT_API_KEY = os.getenv('QDRANT_API_KEY')

# Kh·ªüi t·∫°o Qdrant client
qdrant = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)

# Constants cho file tracking
TRACKING_FOLDER = './tracking'
COLLECTIONS_EXPORT_FOLDER = './exports'


def load_processed_files(collection_name: str) -> Dict:
    """Load danh s√°ch c√°c file ƒë√£ ƒë∆∞·ª£c embedding cho collection"""
    tracking_file = os.path.join(TRACKING_FOLDER, f"{collection_name}_processed.json")
    
    if not os.path.exists(tracking_file):
        return {
            "collection_name": collection_name,
            "created_at": datetime.now().isoformat(),
            "last_updated": datetime.now().isoformat(),
            "processed_files": {}
        }
    
    try:
        with open(tracking_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói ƒë·ªçc file tracking {tracking_file}: {e}")
        return {
            "collection_name": collection_name,
            "created_at": datetime.now().isoformat(),
            "last_updated": datetime.now().isoformat(),
            "processed_files": {}
        }


def save_processed_files(collection_name: str, processed_data: Dict):
    """L∆∞u danh s√°ch c√°c file ƒë√£ ƒë∆∞·ª£c embedding"""
    os.makedirs(TRACKING_FOLDER, exist_ok=True)
    tracking_file = os.path.join(TRACKING_FOLDER, f"{collection_name}_processed.json")
    
    processed_data["last_updated"] = datetime.now().isoformat()
    
    try:
        with open(tracking_file, 'w', encoding='utf-8') as f:
            json.dump(processed_data, f, ensure_ascii=False, indent=2)
        print(f"üíæ ƒê√£ l∆∞u tr·∫°ng th√°i tracking v√†o: {tracking_file}")
    except Exception as e:
        print(f"‚ùå L·ªói l∆∞u file tracking: {e}")


def get_file_hash(file_path: str) -> str:
    """T·∫°o hash t·ª´ file path, size v√† modified time ƒë·ªÉ detect changes"""
    try:
        stat = os.stat(file_path)
        file_info = f"{file_path}_{stat.st_size}_{stat.st_mtime}"
        return str(hash(file_info))
    except Exception:
        return str(hash(file_path))


def is_file_already_processed(file_path: str, processed_files: Dict) -> bool:
    """Ki·ªÉm tra xem file ƒë√£ ƒë∆∞·ª£c embedding ch∆∞a (ch·ªâ ki·ªÉm tra t√™n file)"""
    file_name = os.path.basename(file_path)
    return file_name in processed_files


def mark_file_as_processed(file_path: str, processed_files: Dict, chunks_count: int):
    """ƒê√°nh d·∫•u file ƒë√£ ƒë∆∞·ª£c embedding"""
    file_name = os.path.basename(file_path)
    processed_files[file_name] = {
        "file_path": file_path,
        "file_hash": get_file_hash(file_path),
        "processed_at": datetime.now().isoformat(),
        "chunks_count": chunks_count,
        "file_size": os.path.getsize(file_path)
    }


def embed_text_openai(text: str):
    """G·ªçi API OpenAI ƒë·ªÉ embed text"""
    headers = {
        'Authorization': f'Bearer {OPENAI_API_KEY}',
        'Content-Type': 'application/json'
    }
    data = {"input": text, "model": "text-embedding-3-small"}
    response = requests.post('https://api.openai.com/v1/embeddings', headers=headers, json=data)
    response.raise_for_status()
    return response.json()['data'][0]['embedding']


def simple_chunking(text: str, chunk_size: int = 1000, chunk_overlap: int = 100) -> List[str]:
    """Chunk text ƒë·ªÉ d·ªÖ embedding"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    return splitter.split_text(text)


def create_collection_if_not_exists(collection_name: str, vector_size: int = 1536):
    """T·∫°o collection m·ªõi n·∫øu ch∆∞a t·ªìn t·∫°i"""
    try:
        collections = qdrant.get_collections().collections
        if any(c.name == collection_name for c in collections):
            print(f"‚úÖ Collection '{collection_name}' ƒë√£ t·ªìn t·∫°i, d√πng l·∫°i.")
            return
    except Exception:
        pass

    print(f"üìÇ T·∫°o collection m·ªõi: {collection_name}")
    qdrant.recreate_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)
    )


def find_chunk_pages(chunk: str, full_text: str, page_boundaries: List, chunk_start_in_full_text: int):
    """T√¨m trang b·∫Øt ƒë·∫ßu v√† k·∫øt th√∫c c·ªßa chunk d·ª±a tr√™n v·ªã tr√≠ th·ª±c t·∫ø trong full_text"""
    chunk_end_in_full_text = chunk_start_in_full_text + len(chunk)
    
    start_page = None
    end_page = None
    
    for page_num, page_start, page_end in page_boundaries:
        # T√¨m trang b·∫Øt ƒë·∫ßu: trang c√≥ ch·ª©a k√Ω t·ª± ƒë·∫ßu ti√™n c·ªßa chunk
        if start_page is None and page_start <= chunk_start_in_full_text < page_end:
            start_page = page_num
        
        # T√¨m trang k·∫øt th√∫c: trang c√≥ ch·ª©a k√Ω t·ª± cu·ªëi c√πng c·ªßa chunk
        if page_start < chunk_end_in_full_text <= page_end:
            end_page = page_num
    
    # N·∫øu kh√¥ng t√¨m th·∫•y end_page, c√≥ th·ªÉ chunk k·∫øt th√∫c ·ªü cu·ªëi document
    if end_page is None and start_page is not None:
        end_page = start_page
    
    return start_page, end_page


def process_pdf_to_qdrant(file_path: str, collection_name: str, chunk_size: int = 800, chunk_overlap: int = 200):
    print(f"ƒêang x·ª≠ l√Ω file: {file_path}")

    # ƒê·∫£m b·∫£o collection ƒë√£ t·ªìn t·∫°i
    create_collection_if_not_exists(collection_name)

    doc = fitz.open(file_path)
    full_text = ''
    page_boundaries = []
    offset = 0

    for page_num, page in enumerate(doc):
        text = page.get_text()
        start = offset
        end = start + len(text)
        page_boundaries.append((page_num + 1, start, end))
        full_text += text
        offset = end

    print(f"ƒê√£ ƒë·ªçc {len(doc)} trang, t·ªïng c·ªông {len(full_text)} k√Ω t·ª±")

    # Chunking text
    chunks = simple_chunking(full_text, chunk_size, chunk_overlap)
    print(f"ƒê√£ t·∫°o {len(chunks)} chunks")

    # T√¨m v·ªã tr√≠ th·ª±c t·∫ø c·ªßa t·ª´ng chunk trong full_text
    for i, chunk in enumerate(chunks):
        # T√¨m v·ªã tr√≠ b·∫Øt ƒë·∫ßu c·ªßa chunk trong full_text
        chunk_start_in_full_text = full_text.find(chunk)
        
        # N·∫øu kh√¥ng t√¨m th·∫•y exact match, t√¨m g·∫ßn ƒë√∫ng b·∫±ng c√°ch t√¨m substring ƒë·∫ßu ti√™n
        if chunk_start_in_full_text == -1:
            # Th·ª≠ t√¨m v·ªõi 50 k√Ω t·ª± ƒë·∫ßu c·ªßa chunk
            chunk_preview = chunk[:50] if len(chunk) > 50 else chunk
            chunk_start_in_full_text = full_text.find(chunk_preview)
            
            # N·∫øu v·∫´n kh√¥ng t√¨m th·∫•y, ∆∞·ªõc t√≠nh d·ª±a tr√™n v·ªã tr√≠ chunk
            if chunk_start_in_full_text == -1:
                estimated_start = i * (chunk_size - chunk_overlap)
                chunk_start_in_full_text = min(estimated_start, len(full_text) - len(chunk))

        start_page, end_page = find_chunk_pages(chunk, full_text, page_boundaries, chunk_start_in_full_text)

        print(f"Embedding + Upsert chunk {i + 1}/{len(chunks)} (pages {start_page}-{end_page}): {chunk[:80]}...")

        try:
            vector = embed_text_openai(chunk)
            
            # Upsert ngay t·ª´ng chunk
            qdrant.upsert(
                collection_name=collection_name,
                points=[{
                    'id': str(uuid.uuid4()),
                    'vector': vector,
                    'payload': {
                        'text': chunk,
                        'chunk_index': i,
                        'chunk_size': len(chunk),
                        'start_page': start_page,
                        'end_page': end_page,
                        'source_file': os.path.basename(file_path),
                        'source_path': file_path,
                        'chunk_position': chunk_start_in_full_text  # Th√™m th√¥ng tin v·ªã tr√≠ ƒë·ªÉ debug
                    }
                }]
            )
        except Exception as e:
            print(f"‚ùå L·ªói khi embedding chunk {i}: {e}")
    
    return len(chunks)


def process_documents_for_collection(collection_name: str, base_folder: str = './documents'):
    folder_path = os.path.join(base_folder, collection_name)

    if not os.path.exists(folder_path):
        print(f"‚ùå Folder '{folder_path}' kh√¥ng t·ªìn t·∫°i!")
        return

    pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')]
    if not pdf_files:
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file PDF n√†o trong '{folder_path}'")
        return

    # Load tracking data
    processed_data = load_processed_files(collection_name)
    processed_files = processed_data["processed_files"]

    print(f"üìÅ Found {len(pdf_files)} PDF trong collection '{collection_name}'")
    
    # Ki·ªÉm tra files ƒë√£ ƒë∆∞·ª£c processed
    skipped_files = []
    files_to_process = []
    
    for pdf_file in pdf_files:
        file_path = os.path.join(folder_path, pdf_file)
        print(is_file_already_processed(file_path, processed_files))
        if is_file_already_processed(file_path, processed_files):
            print("ƒê√É B·ªé QUA")
            skipped_files.append(pdf_file)
            print(f"‚è≠Ô∏è B·ªè qua file ƒë√£ embedding: {pdf_file}")
        else:
            files_to_process.append((pdf_file, file_path))

    print(f"\nüìä T·ªïng quan:")
    print(f"  - T·ªïng files: {len(pdf_files)}")
    print(f"  - ƒê√£ embedding: {len(skipped_files)}")
    print(f"  - C·∫ßn embedding: {len(files_to_process)}")

    if skipped_files:
        print(f"\n‚è≠Ô∏è Files ƒë√£ embedding (b·ªè qua):")
        for i, file in enumerate(skipped_files, 1):
            processed_info = processed_files[file]
            print(f"  {i}. {file} (processed: {processed_info['processed_at'][:19]}, chunks: {processed_info['chunks_count']})")

    if not files_to_process:
        print(f"\n‚úÖ T·∫•t c·∫£ files trong collection '{collection_name}' ƒë√£ ƒë∆∞·ª£c embedding!")
        return

    print(f"\nüîÑ Files c·∫ßn embedding:")
    for i, (pdf_file, _) in enumerate(files_to_process, 1):
        print(f"  {i}. {pdf_file}")

    # Process c√°c files ch∆∞a ƒë∆∞·ª£c embedding
    for pdf_file, file_path in files_to_process:
        print(f"\n{'=' * 50}\nProcessing: {pdf_file}\nCollection: {collection_name}\nFull path: {file_path}\n{'=' * 50}")

        try:
            chunks_count = process_pdf_to_qdrant(file_path, collection_name)
            
            # Mark file as processed
            mark_file_as_processed(file_path, processed_files, chunks_count)
            
            # Save tracking data after each file
            save_processed_files(collection_name, processed_data)
            
            print(f"‚úÖ Done: {pdf_file} ({chunks_count} chunks)")
            
        except Exception as e:
            print(f"‚ùå Error processing {pdf_file}: {e}")

    print(f"\nüéâ Ho√†n th√†nh x·ª≠ l√Ω collection '{collection_name}'!")


def export_collection_list(output_folder: str = COLLECTIONS_EXPORT_FOLDER) -> str:
    """Export danh s√°ch t√™n collections ra file JSON cho frontend"""
    try:
        # T·∫°o folder export n·∫øu ch∆∞a c√≥
        os.makedirs(output_folder, exist_ok=True)
        
        # L·∫•y danh s√°ch collections
        collections = qdrant.get_collections().collections
        
        # T·∫°o danh s√°ch t√™n collections
        collection_list = {
            "exported_at": datetime.now().isoformat(),
            "total_collections": len(collections),
            "collections": [collection.name for collection in collections]
        }
        
        # Export ra file JSON
        export_filename = "collections.json"
        export_path = os.path.join(output_folder, export_filename)
        
        with open(export_path, 'w', encoding='utf-8') as f:
            json.dump(collection_list, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ ƒê√£ export danh s√°ch collections ra: {export_path}")
        print(f"üìä T·ªïng c·ªông {len(collections)} collections")
        
        return export_path
        
    except Exception as e:
        print(f"‚ùå L·ªói khi export collections: {e}")
        return None


def show_tracking_status(collection_name: str = None):
    """Hi·ªÉn th·ªã tr·∫°ng th√°i tracking c·ªßa collection(s)"""
    if collection_name:
        # Hi·ªÉn th·ªã cho 1 collection c·ª• th·ªÉ
        processed_data = load_processed_files(collection_name)
        processed_files = processed_data["processed_files"]
        
        print(f"\nüìä Tr·∫°ng th√°i collection '{collection_name}':")
        print(f"  - Created: {processed_data.get('created_at', 'N/A')[:19]}")
        print(f"  - Last updated: {processed_data.get('last_updated', 'N/A')[:19]}")
        print(f"  - Processed files: {len(processed_files)}")
        
        if processed_files:
            print(f"\nüìÅ Files ƒë√£ embedding:")
            for i, (filename, info) in enumerate(processed_files.items(), 1):
                print(f"  {i}. {filename}")
                print(f"     - Size: {info.get('file_size', 0):,} bytes")
                print(f"     - Chunks: {info.get('chunks_count', 0)}")
                print(f"     - Processed: {info.get('processed_at', 'N/A')[:19]}")
    else:
        # Hi·ªÉn th·ªã t·∫•t c·∫£ collections
        if not os.path.exists(TRACKING_FOLDER):
            print("‚ö†Ô∏è Ch∆∞a c√≥ data tracking n√†o!")
            return
            
        tracking_files = [f for f in os.listdir(TRACKING_FOLDER) if f.endswith('_processed.json')]
        
        if not tracking_files:
            print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file tracking n√†o!")
            return
            
        print(f"\nüìä T·ªïng quan tracking ({len(tracking_files)} collections):")
        
        for tracking_file in sorted(tracking_files):
            collection_name = tracking_file.replace('_processed.json', '')
            processed_data = load_processed_files(collection_name)
            processed_files = processed_data["processed_files"]
            
            print(f"\n  üìÅ {collection_name}:")
            print(f"     - Files: {len(processed_files)}")
            print(f"     - Last updated: {processed_data.get('last_updated', 'N/A')[:19]}")


if __name__ == '__main__':
    if len(sys.argv) < 2:
        print("‚ö†Ô∏è H√£y nh·∫≠p l·ªánh c·∫ßn th·ª±c hi·ªán!")
        print("C√°ch s·ª≠ d·ª•ng:")
        print("  python3 embedding.py <collection_name>     - X·ª≠ l√Ω collection")
        print("  python3 embedding.py export               - Export danh s√°ch collections")
        print("  python3 embedding.py status               - Hi·ªÉn th·ªã tr·∫°ng th√°i tracking t·∫•t c·∫£")
        print("  python3 embedding.py status <collection>  - Hi·ªÉn th·ªã tr·∫°ng th√°i collection c·ª• th·ªÉ")
        sys.exit(1)

    command = sys.argv[1]
    
    if command == "export":
        # Export danh s√°ch collections
        export_collection_list()
    elif command == "status":
        # Hi·ªÉn th·ªã tr·∫°ng th√°i tracking
        if len(sys.argv) > 2:
            show_tracking_status(sys.argv[2])
        else:
            show_tracking_status()
    else:
        # Process collection b√¨nh th∆∞·ªùng
        collection_name = command
        process_documents_for_collection(collection_name)